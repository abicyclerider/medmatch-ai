{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Evaluation\n",
    "\n",
    "This notebook evaluates the patient matching pipeline against ground truth data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The MedMatch AI system uses a 4-stage pipeline:\n",
    "1. **Blocking** - Reduces O(n¬≤) comparisons using phonetic and key-based blocking\n",
    "2. **Rules** - Deterministic matching for clear cases (exact matches, MRN matches)\n",
    "3. **Scoring** - Weighted feature scoring for moderate confidence cases\n",
    "4. **AI** - Medical fingerprinting for ambiguous cases using Gemini API\n",
    "\n",
    "## Targets\n",
    "\n",
    "- **Easy cases**: ‚â•95% accuracy\n",
    "- **Medium cases**: ‚â•85% accuracy  \n",
    "- **Hard/ambiguous cases**: ‚â•70% accuracy\n",
    "- **Overall**: ‚â•85% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Evaluation - Phase 2 Complete\n",
    "\n",
    "**Status:** ‚úÖ **Phase 2 Complete** (January 2026)\n",
    "\n",
    "**Overall Results:**\n",
    "\n",
    "- **Accuracy:** 94.51% (437 test pairs)\n",
    "- **All Accuracy Targets Exceeded:** ‚úÖ\n",
    "\n",
    "**Accuracy by Difficulty:**\n",
    "\n",
    "| Difficulty | Target | Achieved | Status |\n",
    "|------------|--------|----------|--------|\n",
    "| Easy       | 95%    | 100.00%  | ‚úÖ PASS |\n",
    "| Medium     | 85%    | 100.00%  | ‚úÖ PASS |\n",
    "| Hard       | 70%    | 88.24%   | ‚úÖ PASS |\n",
    "| Ambiguous  | 70%    | 80.54%   | ‚úÖ PASS |\n",
    "\n",
    "**Pipeline Configuration:**\n",
    "\n",
    "This notebook evaluates the complete 4-stage progressive pipeline:\n",
    "\n",
    "1. **Blocking** - Reduces 33,930 possible pairs ‚Üí 437 candidates (97% reduction, 97.3% recall)\n",
    "2. **Deterministic Rules** - Handles 74% of decisions (92.6% accuracy)\n",
    "3. **Feature Scoring** - Handles 0% of decisions when AI enabled, 26% otherwise (87.6% accuracy)\n",
    "4. **AI Medical Fingerprinting** - Handles 26% of decisions (100% accuracy)\n",
    "\n",
    "**Key Capabilities Demonstrated:**\n",
    "\n",
    "- Medical abbreviation understanding (T2DM = Type 2 Diabetes, HTN = Hypertension)\n",
    "- Name variation handling (John vs Johnny, typos, format differences)\n",
    "- Medical history comparison using Gemini API\n",
    "- Explainable decisions with confidence scores\n",
    "- Progressive pipeline routing based on difficulty\n",
    "\n",
    "**Configuration Notes:**\n",
    "\n",
    "- AI enabled with `api_rate_limit=0` (billing enabled, no rate limiting)\n",
    "- Uses `gemini-2.5-flash` model\n",
    "- Medical records loaded for AI comparison\n",
    "- All 10 integration tests passing\n",
    "\n",
    "**Documentation:**\n",
    "\n",
    "- [Matching Module README](../src/medmatch/matching/README.md) - Complete architecture and usage\n",
    "- [Scripts README](../scripts/README.md) - CLI wrapper for batch processing\n",
    "- [Quick Start Guide](../docs/quickstart.md) - 5-minute getting started guide\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Enable auto-reload of modules (detects code changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MedMatch imports\n",
    "from medmatch.matching import PatientRecord, PatientMatcher, MatchExplainer\n",
    "from medmatch.evaluation import MatchEvaluator, EvaluationMetrics\n",
    "from medmatch.data.models.patient import Demographics, Address\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Setup complete! (Auto-reload enabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the synthetic demographics dataset and convert to PatientRecord objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographics\n",
    "data_dir = project_root / 'data' / 'synthetic'\n",
    "demographics_path = data_dir / 'synthetic_demographics.csv'\n",
    "ground_truth_path = data_dir / 'ground_truth.csv'\n",
    "\n",
    "df_demo = pd.read_csv(demographics_path)\n",
    "df_gt = pd.read_csv(ground_truth_path)\n",
    "\n",
    "print(f\"Loaded {len(df_demo)} demographic records\")\n",
    "print(f\"Loaded {len(df_gt)} ground truth entries\")\n",
    "print(f\"\\nUnique patients: {df_gt['patient_id'].nunique()}\")\n",
    "df_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth difficulty distribution\n",
    "print(\"Difficulty distribution:\")\n",
    "print(df_gt['difficulty'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_patient_records(df: pd.DataFrame, medical_records_path: Path = None) -> list:\n",
    "    \"\"\"Convert demographics DataFrame to PatientRecord list, with optional medical records.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Load medical records if path provided\n",
    "    medical_by_patient = {}\n",
    "    if medical_records_path and medical_records_path.exists():\n",
    "        with open(medical_records_path, 'r') as f:\n",
    "            medical_data = json.load(f)\n",
    "        # Index by patient_id (each patient may have multiple medical records, use first)\n",
    "        for mr in medical_data:\n",
    "            patient_id = mr['patient_id']\n",
    "            if patient_id not in medical_by_patient:\n",
    "                medical_by_patient[patient_id] = mr\n",
    "        print(f\"Loaded {len(medical_by_patient)} medical records\")\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Parse date of birth\n",
    "        dob_str = row['date_of_birth']\n",
    "        if isinstance(dob_str, str):\n",
    "            dob = date.fromisoformat(dob_str)\n",
    "        else:\n",
    "            dob = dob_str\n",
    "        \n",
    "        # Parse record date\n",
    "        rec_date_str = row.get('record_date')\n",
    "        if pd.isna(rec_date_str):\n",
    "            rec_date = date.today()\n",
    "        elif isinstance(rec_date_str, str):\n",
    "            rec_date = date.fromisoformat(rec_date_str.split('T')[0])\n",
    "        else:\n",
    "            rec_date = rec_date_str\n",
    "        \n",
    "        # Create Address if available\n",
    "        address = None\n",
    "        if pd.notna(row.get('address_street')):\n",
    "            address = Address(\n",
    "                street=row['address_street'],\n",
    "                city=row.get('address_city', ''),\n",
    "                state=row.get('address_state', ''),\n",
    "                zip_code=str(row.get('address_zip', '')),\n",
    "            )\n",
    "        \n",
    "        # Create Demographics object\n",
    "        demo = Demographics(\n",
    "            record_id=row['record_id'],\n",
    "            patient_id=row['patient_id'],  # For validation only - not used in matching\n",
    "            name_first=row['name_first'],\n",
    "            name_middle=row.get('name_middle') if pd.notna(row.get('name_middle')) else None,\n",
    "            name_last=row['name_last'],\n",
    "            name_suffix=row.get('name_suffix') if pd.notna(row.get('name_suffix')) else None,\n",
    "            date_of_birth=dob,\n",
    "            gender=row['gender'],\n",
    "            mrn=str(row['mrn']),  # Convert to string for Pydantic validation\n",
    "            ssn_last4=str(row['ssn_last4']) if pd.notna(row.get('ssn_last4')) else None,  # Convert to string\n",
    "            phone=row.get('phone') if pd.notna(row.get('phone')) else None,\n",
    "            email=row.get('email') if pd.notna(row.get('email')) else None,\n",
    "            address=address,\n",
    "            record_source=row.get('record_source', 'unknown'),\n",
    "            record_date=rec_date,\n",
    "            data_quality_flag=row.get('data_quality_flag') if pd.notna(row.get('data_quality_flag')) else None,\n",
    "        )\n",
    "        \n",
    "        # Get medical record for this patient (if available)\n",
    "        medical = None\n",
    "        patient_id = row['patient_id']\n",
    "        if patient_id in medical_by_patient:\n",
    "            mr_data = medical_by_patient[patient_id]\n",
    "            # Import MedicalRecord model\n",
    "            from medmatch.data.models.patient import MedicalRecord, MedicalHistory, MedicalCondition, Surgery\n",
    "            \n",
    "            # Parse medical history\n",
    "            mh_data = mr_data.get('medical_history', {})\n",
    "            conditions = [\n",
    "                MedicalCondition(\n",
    "                    name=c['name'],\n",
    "                    abbreviation=c.get('abbreviation'),\n",
    "                    onset_year=c.get('onset_year'),\n",
    "                    status=c.get('status', 'active')\n",
    "                ) for c in mh_data.get('conditions', [])\n",
    "            ]\n",
    "            surgeries = [\n",
    "                Surgery(\n",
    "                    procedure=s['procedure'],\n",
    "                    date=date.fromisoformat(s['date']) if s.get('date') else None\n",
    "                ) for s in mh_data.get('surgeries', [])\n",
    "            ]\n",
    "            medical_history = MedicalHistory(\n",
    "                conditions=conditions,\n",
    "                medications=mh_data.get('medications', []),\n",
    "                allergies=mh_data.get('allergies', []),\n",
    "                surgeries=surgeries,\n",
    "                family_history=mh_data.get('family_history', []),\n",
    "                social_history=mh_data.get('social_history', '')\n",
    "            )\n",
    "            \n",
    "            # Create MedicalRecord\n",
    "            medical = MedicalRecord(\n",
    "                record_id=mr_data['record_id'],\n",
    "                patient_id=mr_data['patient_id'],\n",
    "                record_source=mr_data.get('record_source', 'unknown'),\n",
    "                record_date=date.fromisoformat(mr_data['record_date'].split('T')[0]),\n",
    "                chief_complaint=mr_data.get('chief_complaint'),\n",
    "                medical_history=medical_history,\n",
    "                assessment=mr_data.get('assessment'),\n",
    "                plan=mr_data.get('plan'),\n",
    "                clinical_notes=mr_data.get('clinical_notes'),\n",
    "            )\n",
    "        \n",
    "        records.append(PatientRecord.from_demographics(demo, medical))\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Load all records WITH medical history\n",
    "medical_records_path = data_dir / 'synthetic_medical_records.json'\n",
    "records = load_patient_records(df_demo, medical_records_path)\n",
    "print(f\"Converted {len(records)} PatientRecord objects\")\n",
    "\n",
    "# Check how many have medical history\n",
    "with_medical = sum(1 for r in records if r.conditions or r.medications)\n",
    "print(f\"Records with medical history: {with_medical}/{len(records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Matcher\n",
    "\n",
    "Set up the PatientMatcher with all stages enabled (except AI by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matcher WITHOUT AI (fast, no API calls)\n",
    "# For production, use ai_backend=\"ollama\" when use_ai=True\n",
    "matcher = PatientMatcher(\n",
    "    use_blocking=True,\n",
    "    use_rules=True,\n",
    "    use_scoring=True,\n",
    "    use_ai=False,  # Set to True to enable AI for ambiguous cases\n",
    ")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = MatchEvaluator(str(ground_truth_path))\n",
    "\n",
    "# Initialize explainer for generating human-readable explanations\n",
    "explainer = MatchExplainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Matching\n",
    "\n",
    "Execute the full matching pipeline on all records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run matching\n",
    "print(\"Running entity resolution pipeline...\")\n",
    "results = matcher.match_datasets(records, show_progress=True)\n",
    "\n",
    "print(f\"\\nGenerated {len(results)} match results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary\n",
    "stats = matcher.get_stats(results)\n",
    "print(\"\\nMatching Statistics:\")\n",
    "print(f\"  Total pairs evaluated: {stats['total_pairs']}\")\n",
    "print(f\"  Matches found: {stats['matches']}\")\n",
    "print(f\"  No-matches: {stats['no_matches']}\")\n",
    "print(f\"  Average confidence: {stats['avg_confidence']:.3f}\")\n",
    "print(f\"\\n  By stage: {stats['by_stage']}\")\n",
    "print(f\"  By type: {stats['by_match_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Results\n",
    "\n",
    "Calculate precision, recall, F1, and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall evaluation\n",
    "overall_metrics = evaluator.evaluate(results)\n",
    "print(overall_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full report\n",
    "report = evaluator.generate_report(results, verbose=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics by Difficulty\n",
    "\n",
    "Compare performance across difficulty levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate by difficulty\n",
    "by_difficulty = evaluator.evaluate_by_difficulty(results)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "difficulty_data = []\n",
    "targets = {'easy': 0.95, 'medium': 0.85, 'hard': 0.70, 'ambiguous': 0.70}\n",
    "\n",
    "for diff in ['easy', 'medium', 'hard', 'ambiguous']:\n",
    "    if diff in by_difficulty:\n",
    "        m = by_difficulty[diff]\n",
    "        difficulty_data.append({\n",
    "            'Difficulty': diff.capitalize(),\n",
    "            'Pairs': m.total_pairs,\n",
    "            'Accuracy': m.accuracy,\n",
    "            'Target': targets[diff],\n",
    "            'Precision': m.precision,\n",
    "            'Recall': m.recall,\n",
    "            'F1': m.f1_score,\n",
    "            'Status': '‚úì PASS' if m.accuracy >= targets[diff] else '‚úó FAIL'\n",
    "        })\n",
    "\n",
    "df_metrics = pd.DataFrame(difficulty_data)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy by difficulty\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(df_metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, df_metrics['Accuracy'], width, label='Actual', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, df_metrics['Target'], width, label='Target', color='coral', alpha=0.7)\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Difficulty Level')\n",
    "ax.set_title('Entity Resolution Accuracy by Difficulty')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_metrics['Difficulty'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.1%}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix\n",
    "\n",
    "Visualize true/false positive/negative distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix\n",
    "cm = np.array([\n",
    "    [overall_metrics.true_negatives, overall_metrics.false_positives],\n",
    "    [overall_metrics.false_negatives, overall_metrics.true_positives]\n",
    "])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted: No Match', 'Predicted: Match'],\n",
    "            yticklabels=['Actual: No Match', 'Actual: Match'],\n",
    "            ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"True Positives:  {overall_metrics.true_positives}\")\n",
    "print(f\"True Negatives:  {overall_metrics.true_negatives}\")\n",
    "print(f\"False Positives: {overall_metrics.false_positives}\")\n",
    "print(f\"False Negatives: {overall_metrics.false_negatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis\n",
    "\n",
    "Examine false positives and false negatives to understand failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all errors\n",
    "errors = evaluator.find_errors(results)\n",
    "print(f\"Total errors: {len(errors)}\")\n",
    "\n",
    "fp_errors = [e for e in errors if e.error_type == 'false_positive']\n",
    "fn_errors = [e for e in errors if e.error_type == 'false_negative']\n",
    "\n",
    "print(f\"False Positives: {len(fp_errors)}\")\n",
    "print(f\"False Negatives: {len(fn_errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze false positives (if any)\n",
    "if fp_errors:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FALSE POSITIVES (predicted match but actually different patients)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, err in enumerate(fp_errors[:5], 1):\n",
    "        print(f\"\\n[{i}] {err.record_1_id} ‚Üî {err.record_2_id}\")\n",
    "        print(f\"    Stage: {err.stage}\")\n",
    "        print(f\"    Confidence: {err.confidence:.2f}\")\n",
    "        print(f\"    Difficulty: {err.difficulty}\")\n",
    "        if err.explanation:\n",
    "            print(f\"    Explanation: {err.explanation}\")\n",
    "else:\n",
    "    print(\"No false positives!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze false negatives (if any)\n",
    "if fn_errors:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FALSE NEGATIVES (predicted no-match but actually same patient)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, err in enumerate(fn_errors[:5], 1):\n",
    "        print(f\"\\n[{i}] {err.record_1_id} ‚Üî {err.record_2_id}\")\n",
    "        print(f\"    Stage: {err.stage}\")\n",
    "        print(f\"    Confidence: {err.confidence:.2f}\")\n",
    "        print(f\"    Difficulty: {err.difficulty}\")\n",
    "        if err.explanation:\n",
    "            print(f\"    Explanation: {err.explanation[:100]}...\")\n",
    "else:\n",
    "    print(\"No false negatives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Matches\n",
    "\n",
    "Show sample match results with full explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample matches\n",
    "matches = [r for r in results if r.is_match]\n",
    "print(f\"Total matches found: {len(matches)}\")\n",
    "\n",
    "if matches:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE MATCH EXPLANATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for result in matches[:3]:\n",
    "        print(\"\\n\" + explainer.explain(result, verbose=True))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample non-matches\n",
    "non_matches = [r for r in results if not r.is_match][:3]\n",
    "\n",
    "if non_matches:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE NON-MATCH EXPLANATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for result in non_matches:\n",
    "        print(\"\\n\" + explainer.explain(result))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Stage Distribution\n",
    "\n",
    "Analyze which pipeline stages are making decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate by stage\n",
    "by_stage = evaluator.evaluate_by_stage(results)\n",
    "\n",
    "stage_data = []\n",
    "for stage, m in sorted(by_stage.items()):\n",
    "    stage_data.append({\n",
    "        'Stage': stage.capitalize(),\n",
    "        'Pairs': m.total_pairs,\n",
    "        'Percentage': m.total_pairs / len(results) * 100,\n",
    "        'Accuracy': m.accuracy,\n",
    "        'Precision': m.precision,\n",
    "        'Recall': m.recall,\n",
    "    })\n",
    "\n",
    "df_stages = pd.DataFrame(stage_data)\n",
    "df_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stage distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart of decision stage distribution\n",
    "ax1.pie(df_stages['Pairs'], labels=df_stages['Stage'], autopct='%1.1f%%',\n",
    "        colors=sns.color_palette('husl', len(df_stages)))\n",
    "ax1.set_title('Decisions by Pipeline Stage')\n",
    "\n",
    "# Bar chart of accuracy by stage\n",
    "ax2.bar(df_stages['Stage'], df_stages['Accuracy'], color='steelblue')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_xlabel('Pipeline Stage')\n",
    "ax2.set_title('Accuracy by Decision Stage')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "for i, v in enumerate(df_stages['Accuracy']):\n",
    "    ax2.text(i, v + 0.02, f'{v:.1%}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Final evaluation summary and target achievement status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 70)\n",
    "print(\"ENTITY RESOLUTION EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Value':>10}\")\n",
    "print(\"-\" * 32)\n",
    "print(f\"{'Total Pairs':<20} {overall_metrics.total_pairs:>10}\")\n",
    "print(f\"{'Accuracy':<20} {overall_metrics.accuracy:>10.2%}\")\n",
    "print(f\"{'Precision':<20} {overall_metrics.precision:>10.2%}\")\n",
    "print(f\"{'Recall':<20} {overall_metrics.recall:>10.2%}\")\n",
    "print(f\"{'F1 Score':<20} {overall_metrics.f1_score:>10.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TARGET ACHIEVEMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_passed = True\n",
    "for diff in ['easy', 'medium', 'hard', 'ambiguous']:\n",
    "    if diff in by_difficulty:\n",
    "        m = by_difficulty[diff]\n",
    "        target = targets[diff]\n",
    "        passed = m.accuracy >= target\n",
    "        status = '‚úì PASS' if passed else '‚úó FAIL'\n",
    "        all_passed = all_passed and passed\n",
    "        print(f\"{diff.capitalize():<12} {m.accuracy:>6.1%} (target: {target:.0%}) [{status}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if all_passed:\n",
    "    print(\"üéâ ALL TARGETS MET - Phase 2.5 Evaluation Complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some targets not met - review error analysis above\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary statistics\n",
    "summary_stats = evaluator.get_summary_stats(results)\n",
    "\n",
    "# Save to JSON for programmatic access\n",
    "import json\n",
    "output_path = project_root / 'data' / 'synthetic' / 'evaluation_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "print(f\"Summary stats saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional: Run with AI Enabled\n",
    "\n",
    "To enable AI medical fingerprinting for ambiguous cases, uncomment and run the cell below.\n",
    "**Note:** This requires a valid `GOOGLE_AI_API_KEY` in your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test AI is working - run this cell to verify\n#\n# Backend Options:\n# - \"ollama\": Local MedGemma (HIPAA-compliant, recommended for production)\n# - \"gemini\": Cloud API (development/testing only, requires GOOGLE_AI_API_KEY)\n\nmatcher_with_ai = PatientMatcher(\n    use_blocking=True,\n    use_rules=True,\n    use_scoring=True,\n    use_ai=True,\n    ai_backend=\"ollama\",  # Use local MedGemma via Ollama\n    # ai_backend=\"gemini\",  # Uncomment to use Gemini API instead\n    model=\"medgemma:1.5-4b-q4\",  # Quantized model (2-4x faster!)\n    api_rate_limit=0,  # No rate limiting\n)\n\nprint(\"Running with AI enabled (backend: ollama, model: medgemma:1.5-4b-q4)...\")\nresults_with_ai = matcher_with_ai.match_datasets(records, show_progress=True)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}