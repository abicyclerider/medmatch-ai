{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Evaluation\n",
    "\n",
    "This notebook evaluates the patient matching pipeline against ground truth data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The MedMatch AI system uses a 4-stage pipeline:\n",
    "1. **Blocking** - Reduces O(n¬≤) comparisons using phonetic and key-based blocking\n",
    "2. **Rules** - Deterministic matching for clear cases (exact matches, MRN matches)\n",
    "3. **Scoring** - Weighted feature scoring for moderate confidence cases\n",
    "4. **AI** - Medical fingerprinting for ambiguous cases using Gemini API\n",
    "\n",
    "## Targets\n",
    "\n",
    "- **Easy cases**: ‚â•95% accuracy\n",
    "- **Medium cases**: ‚â•85% accuracy  \n",
    "- **Hard/ambiguous cases**: ‚â•70% accuracy\n",
    "- **Overall**: ‚â•85% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MedMatch imports\n",
    "from medmatch.matching import PatientRecord, PatientMatcher, MatchExplainer\n",
    "from medmatch.evaluation import MatchEvaluator, EvaluationMetrics\n",
    "from medmatch.data.models.patient import Demographics, Address\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the synthetic demographics dataset and convert to PatientRecord objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographics\n",
    "data_dir = project_root / 'data' / 'synthetic'\n",
    "demographics_path = data_dir / 'synthetic_demographics.csv'\n",
    "ground_truth_path = data_dir / 'ground_truth.csv'\n",
    "\n",
    "df_demo = pd.read_csv(demographics_path)\n",
    "df_gt = pd.read_csv(ground_truth_path)\n",
    "\n",
    "print(f\"Loaded {len(df_demo)} demographic records\")\n",
    "print(f\"Loaded {len(df_gt)} ground truth entries\")\n",
    "print(f\"\\nUnique patients: {df_gt['patient_id'].nunique()}\")\n",
    "df_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth difficulty distribution\n",
    "print(\"Difficulty distribution:\")\n",
    "print(df_gt['difficulty'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patient_records(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Convert demographics DataFrame to PatientRecord list.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Parse date of birth\n",
    "        dob_str = row['date_of_birth']\n",
    "        if isinstance(dob_str, str):\n",
    "            dob = date.fromisoformat(dob_str)\n",
    "        else:\n",
    "            dob = dob_str\n",
    "        \n",
    "        # Parse record date\n",
    "        rec_date_str = row.get('record_date')\n",
    "        if pd.isna(rec_date_str):\n",
    "            rec_date = date.today()\n",
    "        elif isinstance(rec_date_str, str):\n",
    "            rec_date = date.fromisoformat(rec_date_str.split('T')[0])\n",
    "        else:\n",
    "            rec_date = rec_date_str\n",
    "        \n",
    "        # Create Address if available\n",
    "        address = None\n",
    "        if pd.notna(row.get('street')):\n",
    "            address = Address(\n",
    "                street=row['street'],\n",
    "                city=row.get('city', ''),\n",
    "                state=row.get('state', ''),\n",
    "                zip_code=row.get('zip_code', ''),\n",
    "            )\n",
    "        \n",
    "        # Create Demographics object\n",
    "        demo = Demographics(\n",
    "            record_id=row['record_id'],\n",
    "            patient_id=row['patient_id'],  # For validation only - not used in matching\n",
    "            name_first=row['name_first'],\n",
    "            name_middle=row.get('name_middle') if pd.notna(row.get('name_middle')) else None,\n",
    "            name_last=row['name_last'],\n",
    "            name_suffix=row.get('name_suffix') if pd.notna(row.get('name_suffix')) else None,\n",
    "            date_of_birth=dob,\n",
    "            gender=row['gender'],\n",
    "            mrn=row['mrn'],\n",
    "            ssn_last4=row.get('ssn_last4') if pd.notna(row.get('ssn_last4')) else None,\n",
    "            phone=row.get('phone') if pd.notna(row.get('phone')) else None,\n",
    "            email=row.get('email') if pd.notna(row.get('email')) else None,\n",
    "            address=address,\n",
    "            record_source=row.get('record_source', 'unknown'),\n",
    "            record_date=rec_date,\n",
    "            data_quality_flag=row.get('data_quality_flag') if pd.notna(row.get('data_quality_flag')) else None,\n",
    "        )\n",
    "        \n",
    "        records.append(PatientRecord.from_demographics(demo))\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Load all records\n",
    "records = load_patient_records(df_demo)\n",
    "print(f\"Converted {len(records)} PatientRecord objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Matcher\n",
    "\n",
    "Set up the PatientMatcher with all stages enabled (except AI by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matcher WITHOUT AI (fast, no API calls)\n",
    "matcher = PatientMatcher(\n",
    "    use_blocking=True,\n",
    "    use_rules=True,\n",
    "    use_scoring=True,\n",
    "    use_ai=False,  # Set to True to enable AI for ambiguous cases\n",
    ")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = MatchEvaluator(str(ground_truth_path))\n",
    "\n",
    "# Initialize explainer\n",
    "explainer = MatchExplainer()\n",
    "\n",
    "print(\"Matcher initialized with stages: blocking + rules + scoring\")\n",
    "print(f\"Ground truth has {len(evaluator.get_all_true_match_pairs())} true match pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Matching\n",
    "\n",
    "Execute the full matching pipeline on all records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run matching\n",
    "print(\"Running entity resolution pipeline...\")\n",
    "results = matcher.match_datasets(records, show_progress=True)\n",
    "\n",
    "print(f\"\\nGenerated {len(results)} match results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary\n",
    "stats = matcher.get_statistics(results)\n",
    "print(\"\\nMatching Statistics:\")\n",
    "print(f\"  Total pairs evaluated: {stats['total_pairs']}\")\n",
    "print(f\"  Matches found: {stats['matches']}\")\n",
    "print(f\"  Non-matches: {stats['no_matches']}\")\n",
    "print(f\"  Average confidence: {stats['avg_confidence']:.3f}\")\n",
    "print(f\"\\n  By stage: {stats['by_stage']}\")\n",
    "print(f\"  By type: {stats['by_match_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Results\n",
    "\n",
    "Calculate precision, recall, F1, and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall evaluation\n",
    "overall_metrics = evaluator.evaluate(results)\n",
    "print(overall_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full report\n",
    "report = evaluator.generate_report(results, verbose=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics by Difficulty\n",
    "\n",
    "Compare performance across difficulty levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate by difficulty\n",
    "by_difficulty = evaluator.evaluate_by_difficulty(results)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "difficulty_data = []\n",
    "targets = {'easy': 0.95, 'medium': 0.85, 'hard': 0.70, 'ambiguous': 0.70}\n",
    "\n",
    "for diff in ['easy', 'medium', 'hard', 'ambiguous']:\n",
    "    if diff in by_difficulty:\n",
    "        m = by_difficulty[diff]\n",
    "        difficulty_data.append({\n",
    "            'Difficulty': diff.capitalize(),\n",
    "            'Pairs': m.total_pairs,\n",
    "            'Accuracy': m.accuracy,\n",
    "            'Target': targets[diff],\n",
    "            'Precision': m.precision,\n",
    "            'Recall': m.recall,\n",
    "            'F1': m.f1_score,\n",
    "            'Status': '‚úì PASS' if m.accuracy >= targets[diff] else '‚úó FAIL'\n",
    "        })\n",
    "\n",
    "df_metrics = pd.DataFrame(difficulty_data)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy by difficulty\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(df_metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, df_metrics['Accuracy'], width, label='Actual', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, df_metrics['Target'], width, label='Target', color='coral', alpha=0.7)\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Difficulty Level')\n",
    "ax.set_title('Entity Resolution Accuracy by Difficulty')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_metrics['Difficulty'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.1%}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix\n",
    "\n",
    "Visualize true/false positive/negative distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix\n",
    "cm = np.array([\n",
    "    [overall_metrics.true_negatives, overall_metrics.false_positives],\n",
    "    [overall_metrics.false_negatives, overall_metrics.true_positives]\n",
    "])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted: No Match', 'Predicted: Match'],\n",
    "            yticklabels=['Actual: No Match', 'Actual: Match'],\n",
    "            ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"True Positives:  {overall_metrics.true_positives}\")\n",
    "print(f\"True Negatives:  {overall_metrics.true_negatives}\")\n",
    "print(f\"False Positives: {overall_metrics.false_positives}\")\n",
    "print(f\"False Negatives: {overall_metrics.false_negatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis\n",
    "\n",
    "Examine false positives and false negatives to understand failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all errors\n",
    "errors = evaluator.find_errors(results)\n",
    "print(f\"Total errors: {len(errors)}\")\n",
    "\n",
    "fp_errors = [e for e in errors if e.error_type == 'false_positive']\n",
    "fn_errors = [e for e in errors if e.error_type == 'false_negative']\n",
    "\n",
    "print(f\"False Positives: {len(fp_errors)}\")\n",
    "print(f\"False Negatives: {len(fn_errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze false positives (if any)\n",
    "if fp_errors:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FALSE POSITIVES (predicted match but actually different patients)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, err in enumerate(fp_errors[:5], 1):\n",
    "        print(f\"\\n[{i}] {err.record_1_id} ‚Üî {err.record_2_id}\")\n",
    "        print(f\"    Stage: {err.stage}\")\n",
    "        print(f\"    Confidence: {err.confidence:.2f}\")\n",
    "        print(f\"    Difficulty: {err.difficulty}\")\n",
    "        if err.explanation:\n",
    "            print(f\"    Explanation: {err.explanation[:100]}...\")\n",
    "else:\n",
    "    print(\"No false positives!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze false negatives (if any)\n",
    "if fn_errors:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FALSE NEGATIVES (predicted no-match but actually same patient)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, err in enumerate(fn_errors[:5], 1):\n",
    "        print(f\"\\n[{i}] {err.record_1_id} ‚Üî {err.record_2_id}\")\n",
    "        print(f\"    Stage: {err.stage}\")\n",
    "        print(f\"    Confidence: {err.confidence:.2f}\")\n",
    "        print(f\"    Difficulty: {err.difficulty}\")\n",
    "        if err.explanation:\n",
    "            print(f\"    Explanation: {err.explanation[:100]}...\")\n",
    "else:\n",
    "    print(\"No false negatives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Matches\n",
    "\n",
    "Show sample match results with full explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample matches\n",
    "matches = [r for r in results if r.is_match]\n",
    "print(f\"Total matches found: {len(matches)}\")\n",
    "\n",
    "if matches:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE MATCH EXPLANATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for result in matches[:3]:\n",
    "        print(\"\\n\" + explainer.explain(result, verbose=True))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample non-matches\n",
    "non_matches = [r for r in results if not r.is_match][:3]\n",
    "\n",
    "if non_matches:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE NON-MATCH EXPLANATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for result in non_matches:\n",
    "        print(\"\\n\" + explainer.explain(result))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Stage Distribution\n",
    "\n",
    "Analyze which pipeline stages are making decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate by stage\n",
    "by_stage = evaluator.evaluate_by_stage(results)\n",
    "\n",
    "stage_data = []\n",
    "for stage, m in sorted(by_stage.items()):\n",
    "    stage_data.append({\n",
    "        'Stage': stage.capitalize(),\n",
    "        'Pairs': m.total_pairs,\n",
    "        'Percentage': m.total_pairs / len(results) * 100,\n",
    "        'Accuracy': m.accuracy,\n",
    "        'Precision': m.precision,\n",
    "        'Recall': m.recall,\n",
    "    })\n",
    "\n",
    "df_stages = pd.DataFrame(stage_data)\n",
    "df_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stage distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart of decision stage distribution\n",
    "ax1.pie(df_stages['Pairs'], labels=df_stages['Stage'], autopct='%1.1f%%',\n",
    "        colors=sns.color_palette('husl', len(df_stages)))\n",
    "ax1.set_title('Decisions by Pipeline Stage')\n",
    "\n",
    "# Bar chart of accuracy by stage\n",
    "ax2.bar(df_stages['Stage'], df_stages['Accuracy'], color='steelblue')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_xlabel('Pipeline Stage')\n",
    "ax2.set_title('Accuracy by Decision Stage')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "for i, v in enumerate(df_stages['Accuracy']):\n",
    "    ax2.text(i, v + 0.02, f'{v:.1%}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Final evaluation summary and target achievement status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 70)\n",
    "print(\"ENTITY RESOLUTION EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Value':>10}\")\n",
    "print(\"-\" * 32)\n",
    "print(f\"{'Total Pairs':<20} {overall_metrics.total_pairs:>10}\")\n",
    "print(f\"{'Accuracy':<20} {overall_metrics.accuracy:>10.2%}\")\n",
    "print(f\"{'Precision':<20} {overall_metrics.precision:>10.2%}\")\n",
    "print(f\"{'Recall':<20} {overall_metrics.recall:>10.2%}\")\n",
    "print(f\"{'F1 Score':<20} {overall_metrics.f1_score:>10.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TARGET ACHIEVEMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_passed = True\n",
    "for diff in ['easy', 'medium', 'hard', 'ambiguous']:\n",
    "    if diff in by_difficulty:\n",
    "        m = by_difficulty[diff]\n",
    "        target = targets[diff]\n",
    "        passed = m.accuracy >= target\n",
    "        status = '‚úì PASS' if passed else '‚úó FAIL'\n",
    "        all_passed = all_passed and passed\n",
    "        print(f\"{diff.capitalize():<12} {m.accuracy:>6.1%} (target: {target:.0%}) [{status}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if all_passed:\n",
    "    print(\"üéâ ALL TARGETS MET - Phase 2.5 Evaluation Complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some targets not met - review error analysis above\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary statistics\n",
    "summary_stats = evaluator.get_summary_stats(results)\n",
    "\n",
    "# Save to JSON for programmatic access\n",
    "import json\n",
    "output_path = project_root / 'data' / 'synthetic' / 'evaluation_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "print(f\"Summary stats saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional: Run with AI Enabled\n",
    "\n",
    "To enable AI medical fingerprinting for ambiguous cases, uncomment and run the cell below.\n",
    "**Note:** This requires a valid `GOOGLE_AI_API_KEY` in your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to run with AI enabled\n",
    "# matcher_with_ai = PatientMatcher(\n",
    "#     use_blocking=True,\n",
    "#     use_rules=True,\n",
    "#     use_scoring=True,\n",
    "#     use_ai=True,\n",
    "#     api_rate_limit=0,  # No rate limiting (billing enabled)\n",
    "# )\n",
    "# \n",
    "# print(\"Running with AI enabled (may take a few minutes for ambiguous cases)...\")\n",
    "# results_with_ai = matcher_with_ai.match_datasets(records, show_progress=True)\n",
    "# \n",
    "# # Evaluate\n",
    "# metrics_with_ai = evaluator.evaluate(results_with_ai)\n",
    "# print(f\"\\nWith AI - Accuracy: {metrics_with_ai.accuracy:.2%}\")\n",
    "# \n",
    "# # Compare stages\n",
    "# by_stage_ai = evaluator.evaluate_by_stage(results_with_ai)\n",
    "# for stage, m in sorted(by_stage_ai.items()):\n",
    "#     print(f\"  {stage}: {m.total_pairs} pairs, {m.accuracy:.2%} accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
